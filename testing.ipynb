{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from fastervit import create_model\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the number of classes in your custom dataset\n",
    "num_classes = 10\n",
    "\n",
    "# Create the model architecture\n",
    "model = create_model('faster_vit_3_224', pretrained=False)\n",
    "\n",
    "# Modify the final classification layer\n",
    "model.head = torch.nn.Linear(model.head.in_features, num_classes)\n",
    "\n",
    "# Move the model to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the trained model weights\n",
    "model.load_state_dict(torch.load('/kaggle/input/mode-4/faster_vit3_normal_final.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Define data transformations\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to load and preprocess the image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = preprocess(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image.to(device)\n",
    "\n",
    "# Function to make predictions\n",
    "def predict(image_path, model):\n",
    "    image = load_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "    return probabilities.squeeze().tolist()\n",
    "\n",
    "# List of class names\n",
    "class_names = ['Angioectasia', 'Bleeding', 'Erosion', 'Erythema', 'Foreign Body',\n",
    "               'Lymphangiectasia', 'Normal', 'Polyp', 'Ulcer', 'Worms']\n",
    "\n",
    "# Directory containing test images\n",
    "test_dir = '/kaggle/input/test-data/Testing set/Images'  # Update this path\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Process each image in the test directory\n",
    "for filename in tqdm(os.listdir(test_dir)):\n",
    "    if filename.endswith('.jpg'):\n",
    "        image_path = os.path.join(test_dir, filename)\n",
    "        probabilities = predict(image_path, model)\n",
    "        \n",
    "        # Find the predicted class\n",
    "        predicted_class = class_names[probabilities.index(max(probabilities))]\n",
    "        \n",
    "        # Add results to the list\n",
    "        results.append({\n",
    "            'image_path': filename,\n",
    "            **{class_name: prob for class_name, prob in zip(class_names, probabilities)},\n",
    "            'predicted_class': predicted_class\n",
    "        })\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Reorder columns to match the desired format\n",
    "column_order = ['image_path'] + class_names + ['predicted_class']\n",
    "df = df[column_order]\n",
    "\n",
    "# Save to Excel\n",
    "output_file = 'test_prediction_results.xlsx'\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from fastervit import create_model\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, recall_score, f1_score, balanced_accuracy_score\n",
    "# Define metrics calculation functions\n",
    "def calculate_specificity(y_true, y_pred):\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    return specificity\n",
    "\n",
    "def generate_metrics_report(y_true, y_pred):\n",
    "    class_columns = ['Angioectasia', 'Bleeding', 'Erosion', 'Erythema', 'Foreign Body', \n",
    "                    'Lymphangiectasia', 'Normal', 'Polyp', 'Ulcer', 'Worms']\n",
    "    metrics_report = {}\n",
    "    \n",
    "    y_true_classes = np.argmax(y_true, axis=1)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    class_report = classification_report(y_true_classes, y_pred_classes, \n",
    "                                      target_names=class_columns, \n",
    "                                      output_dict=True, \n",
    "                                      zero_division=0)\n",
    "    \n",
    "    # Calculate AUC-ROC scores\n",
    "    auc_roc_scores = {}\n",
    "    for i, class_name in enumerate(class_columns):\n",
    "        try:\n",
    "            auc_roc_scores[class_name] = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "        except ValueError:\n",
    "            auc_roc_scores[class_name] = 0.0\n",
    "    \n",
    "    mean_auc_roc = np.mean(list(auc_roc_scores.values()))\n",
    "    auc_roc_scores['mean_auc'] = mean_auc_roc\n",
    "    \n",
    "    # Calculate specificity scores\n",
    "    specificity_scores = {}\n",
    "    for i, class_name in enumerate(class_columns):\n",
    "        specificity_scores[class_name] = calculate_specificity(y_true[:, i], \n",
    "                                                             (y_pred[:, i] >= 0.5).astype(int))\n",
    "    \n",
    "    mean_specificity = np.mean(list(specificity_scores.values()))\n",
    "    specificity_scores['mean_specificity'] = mean_specificity\n",
    "    \n",
    "    # Calculate average precision scores\n",
    "    average_precision_scores = {}\n",
    "    for i, class_name in enumerate(class_columns):\n",
    "        try:\n",
    "            precision, recall, _ = precision_recall_curve(y_true[:, i], y_pred[:, i])\n",
    "            average_precision_scores[class_name] = auc(recall, precision)\n",
    "        except ValueError:\n",
    "            average_precision_scores[class_name] = 0.0\n",
    "    \n",
    "    mean_average_precision = np.mean(list(average_precision_scores.values()))\n",
    "    average_precision_scores['mean_average_precision'] = mean_average_precision\n",
    "    \n",
    "    # Calculate sensitivity scores\n",
    "    sensitivity_scores = {}\n",
    "    for i, class_name in enumerate(class_columns):\n",
    "        try:\n",
    "            sensitivity_scores[class_name] = recall_score(y_true[:, i], \n",
    "                                                        (y_pred[:, i] >= 0.5).astype(int), \n",
    "                                                        zero_division=0)\n",
    "        except ValueError:\n",
    "            sensitivity_scores[class_name] = 0.0\n",
    "    \n",
    "    mean_sensitivity = np.mean(list(sensitivity_scores.values()))\n",
    "    sensitivity_scores['mean_sensitivity'] = mean_sensitivity\n",
    "    \n",
    "    # Calculate F1 scores\n",
    "    f1_scores = {}\n",
    "    for i, class_name in enumerate(class_columns):\n",
    "        try:\n",
    "            f1_scores[class_name] = f1_score(y_true[:, i], \n",
    "                                           (y_pred[:, i] >= 0.5).astype(int), \n",
    "                                           zero_division=0)\n",
    "        except ValueError:\n",
    "            f1_scores[class_name] = 0.0\n",
    "    \n",
    "    mean_f1_score = np.mean(list(f1_scores.values()))\n",
    "    f1_scores['mean_f1_score'] = mean_f1_score\n",
    "    \n",
    "    # Calculate balanced accuracy\n",
    "    balanced_accuracy_scores = balanced_accuracy_score(y_true_classes, y_pred_classes)\n",
    "    \n",
    "    # Compile all metrics\n",
    "    metrics_report.update(class_report)\n",
    "    metrics_report['auc_roc_scores'] = auc_roc_scores\n",
    "    metrics_report['specificity_scores'] = specificity_scores\n",
    "    metrics_report['average_precision_scores'] = average_precision_scores\n",
    "    metrics_report['sensitivity_scores'] = sensitivity_scores\n",
    "    metrics_report['f1_scores'] = f1_scores\n",
    "    metrics_report['mean_auc'] = mean_auc_roc\n",
    "    metrics_report['mean_specificity'] = mean_specificity\n",
    "    metrics_report['mean_average_precision'] = mean_average_precision\n",
    "    metrics_report['mean_sensitivity'] = mean_sensitivity\n",
    "    metrics_report['mean_f1_score'] = mean_f1_score\n",
    "    metrics_report['balanced_accuracy'] = balanced_accuracy_scores\n",
    "    \n",
    "    metrics_report_json = json.dumps(metrics_report, indent=4)\n",
    "    return metrics_report_json\n",
    "\n",
    "# Define the number of classes in your custom dataset\n",
    "num_classes = 10\n",
    "\n",
    "# Create the model architecture\n",
    "model = create_model('faster_vit_3_224', pretrained=False)\n",
    "# Modify the final classification layer\n",
    "model.head = torch.nn.Linear(model.head.in_features, num_classes)\n",
    "\n",
    "# Move the model to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the trained model weights\n",
    "model.load_state_dict(torch.load('/kaggle/input/mode-4/faster_vit3_normal_final.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Define data transformations\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to load and preprocess the image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = preprocess(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image.to(device)\n",
    "\n",
    "# Function to make predictions\n",
    "def predict(image_path, model, class_names):\n",
    "    image = load_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        predicted_class = class_names[probabilities.argmax().item()]\n",
    "    return probabilities.squeeze().tolist(), predicted_class\n",
    "\n",
    "# List of class names\n",
    "class_names = ['Angioectasia', 'Bleeding', 'Erosion', 'Erythema', 'Foreign Body',\n",
    "               'Lymphangiectasia', 'Normal', 'Polyp', 'Ulcer', 'Worms']\n",
    "\n",
    "# Base directory\n",
    "base_dir = '/kaggle/input/example/example/Dataset/val'\n",
    "\n",
    "# Initialize lists to store true labels and predictions\n",
    "y_true_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Initialize the DataFrame\n",
    "data = []\n",
    "\n",
    "# Recursively traverse the directory structure with progress bar\n",
    "for root, dirs, files in tqdm(os.walk(base_dir), desc=\"Processing images\"):\n",
    "    if files:  # Only process if there are files in the directory\n",
    "        # Get the true class from the directory name\n",
    "        true_class = os.path.basename(root)\n",
    "        if true_class in class_names:  # Make sure it's a valid class directory\n",
    "            for file in files:\n",
    "                if file.endswith('.jpg'):\n",
    "                    # Construct the relative image path\n",
    "                    image_path = os.path.join(root, file).replace(base_dir + '\\\\', '')\n",
    "                    \n",
    "                    # Make the prediction\n",
    "                    probabilities, predicted_class = predict(os.path.join(root, file), model, class_names)\n",
    "                    \n",
    "                    # Create one-hot encoded true label\n",
    "                    true_label = np.zeros(len(class_names))\n",
    "                    true_label[class_names.index(true_class)] = 1\n",
    "                    \n",
    "                    # Add to lists for metrics calculation\n",
    "                    y_true_list.append(true_label)\n",
    "                    y_pred_list.append(probabilities)\n",
    "                    \n",
    "                    # Add the row to the data list\n",
    "                    data.append({\n",
    "                        'image_path': image_path,\n",
    "                        'true_class': true_class,\n",
    "                        **{class_name: prob for class_name, prob in zip(class_names, probabilities)},\n",
    "                        'predicted_class': predicted_class\n",
    "                    })\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_true = np.array(y_true_list)\n",
    "y_pred = np.array(y_pred_list)\n",
    "\n",
    "# Generate metrics report\n",
    "metrics_report = generate_metrics_report(y_true, y_pred)\n",
    "\n",
    "# Create the DataFrame and save it to Excel\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel('validation_prediction_results.xlsx', index=False)\n",
    "\n",
    "# Save metrics report to JSON\n",
    "with open('metrics_report.json', 'w') as f:\n",
    "    f.write(metrics_report)\n",
    "\n",
    "# Print metrics report\n",
    "print(\"\\nMetrics Report:\")\n",
    "print(metrics_report)\n",
    "\n",
    "print(\"\\nResults saved to predictions_output.xlsx and metrics_report.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
